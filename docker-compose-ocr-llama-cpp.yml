x-model-repo: &model_repo Qwen/Qwen3-VL-8B-Instruct-GGUF
x-model-file: &model_file Qwen3VL-8B-Instruct-Q8_0.gguf
x-mmproj-file: &mmproj_file mmproj-Qwen3VL-8B-Instruct-Q8_0.gguf
x-model-path: &model_path /models/Qwen3VL-8B-Instruct-Q8_0.gguf
x-mmproj-path: &mmproj_path /models/mmproj-Qwen3VL-8B-Instruct-Q8_0.gguf
x-remote-ocr-model: &remote_ocr_model Qwen3VL-8B-Instruct-Q8_0

services:
  model-downloader:
    image: curlimages/curl:latest
    container_name: model-downloader
    user: root
    volumes:
      - ./models:/models
    environment:
      MODEL_REPO: *model_repo
      MODEL_FILE: *model_file
      MMPROJ_FILE: *mmproj_file
    entrypoint: sh
    command:
      - -c
      - |
        cd /models
        BASE_URL="https://huggingface.co/$${MODEL_REPO}/resolve/main"

        if [ ! -f "$${MODEL_FILE}" ]; then
          echo "Downloading LLM model..."
          curl -L --progress-bar -o "$${MODEL_FILE}" "$${BASE_URL}/$${MODEL_FILE}"
        else
          echo "LLM model already exists, skipping."
        fi

        if [ ! -f "$${MMPROJ_FILE}" ]; then
          echo "Downloading vision encoder..."
          curl -L --progress-bar -o "$${MMPROJ_FILE}" "$${BASE_URL}/$${MMPROJ_FILE}"
        else
          echo "Vision encoder already exists, skipping."
        fi

        echo "All models ready!"

  llama-cpp-ocr:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-cpp-ocr
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "3000:8080"
    volumes:
      - ./models:/models
    environment:
      LLAMA_ARG_MODEL: *model_path
      LLAMA_ARG_N_GPU_LAYERS: 99
      LLAMA_ARG_CTX_SIZE: 65536
      LLAMA_ARG_N_PARALLEL: ${LLAMA_N_PARALLEL:-16}
      LLAMA_ARG_HOST: 0.0.0.0
      LLAMA_ARG_PORT: 8080
      LLAMA_ARG_CONT_BATCHING: 1
      LLAMA_ARG_ENDPOINT_METRICS: 1
      # Лимит генерации на запрос: защита от бесконечной генерации
      LLAMA_ARG_N_PREDICT: 2048
    # Параметры, не имеющие документированных env-переменных, передаются через command.
    # Sampling defaults (рекомендации Qwen3-VL): применяются, если клиент не передаёт их в запросе.
    command:
      - "--mmproj"
      - *mmproj_path
      - "--top-k"
      - "20"
      - "--top-p"
      - "0.8"
      - "--presence-penalty"
      - "1.5"
    networks:
      - pdf-analysis-network

  pdf-document-layout-analysis-gpu:
    container_name: pdf-document-layout-analysis-gpu
    entrypoint: [ "gunicorn", "-k", "uvicorn.workers.UvicornWorker", "--chdir", "./src", "app:app", "--bind", "0.0.0.0:5060", "--timeout", "10000"]
    init: true
    restart: unless-stopped
    build:
      context: .
      dockerfile: Dockerfile
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "5060:5060"
    environment:
      RESTART_IF_NO_GPU: ${RESTART_IF_NO_GPU:-false}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      REMOTE_OCR_ENABLED: "true"
      REMOTE_OCR_BASE_URL: http://llama-cpp-ocr:8080/v1
      REMOTE_OCR_API_KEY: "not-needed"
      REMOTE_OCR_MODEL: *remote_ocr_model
      REMOTE_OCR_TEMPERATURE: "0.7"
      REMOTE_OCR_TIMEOUT_SEC: "300"
      REMOTE_OCR_MAX_CONCURRENCY: ${REMOTE_OCR_MAX_CONCURRENCY:-64}
    networks:
      - pdf-analysis-network

networks:
  pdf-analysis-network:
    driver: bridge

