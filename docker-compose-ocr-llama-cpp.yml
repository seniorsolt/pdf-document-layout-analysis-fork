# Qwen3-VL-8B via llama.cpp server (OpenAI-compatible API)
#
# При первом запуске init-контейнер автоматически скачивает модель и mmproj
# с HuggingFace (~10 GB). Файлы сохраняются в ./models/,
# повторные запуски не требуют повторного скачивания.
#
# Параллелизм: LLAMA_N_PARALLEL (по умолчанию 2).
# Для NVIDIA P40 (24GB) рекомендуется 2-4 слота с ctx-size 4096-8192.
#

x-model-repo: &model_repo Qwen/Qwen3-VL-8B-Instruct-GGUF
x-model-file: &model_file Qwen3VL-8B-Instruct-Q8_0.gguf
x-mmproj-file: &mmproj_file mmproj-Qwen3VL-8B-Instruct-Q8_0.gguf
x-model-path: &model_path /models/Qwen3VL-8B-Instruct-Q8_0.gguf
x-mmproj-path: &mmproj_path /models/mmproj-Qwen3VL-8B-Instruct-Q8_0.gguf
x-remote-ocr-model: &remote_ocr_model Qwen3VL-8B-Instruct-Q8_0

services:
  model-downloader:
    image: curlimages/curl:latest
    container_name: model-downloader
    volumes:
      - ./models:/models
    environment:
      MODEL_REPO: *model_repo
      MODEL_FILE: *model_file
      MMPROJ_FILE: *mmproj_file
    entrypoint: sh
    command:
      - -c
      - |
        cd /models
        BASE_URL="https://huggingface.co/$${MODEL_REPO}/resolve/main"

        if [ ! -f "$${MODEL_FILE}" ]; then
          echo "Downloading LLM model..."
          curl -L --progress-bar -o "$${MODEL_FILE}" "$${BASE_URL}/$${MODEL_FILE}"
        else
          echo "LLM model already exists, skipping."
        fi

        if [ ! -f "$${MMPROJ_FILE}" ]; then
          echo "Downloading vision encoder..."
          curl -L --progress-bar -o "$${MMPROJ_FILE}" "$${BASE_URL}/$${MMPROJ_FILE}"
        else
          echo "Vision encoder already exists, skipping."
        fi

        echo "All models ready!"

  llama-cpp-ocr:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-cpp-ocr
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "3001:8080"
    volumes:
      - ./models:/models
    environment:
      LLAMA_ARG_MODEL: *model_path
      LLAMA_ARG_N_GPU_LAYERS: 99
      LLAMA_ARG_CTX_SIZE: 8192
      LLAMA_ARG_N_PARALLEL: ${LLAMA_N_PARALLEL:-2}
      LLAMA_ARG_HOST: 0.0.0.0
      LLAMA_ARG_PORT: 8080
      # Flash Attention: по умолчанию "auto" (включается автоматически).
      # P40 (Pascal, CC 6.1) не поддерживает FA-ядра в llama.cpp — явно отключаем.
      LLAMA_ARG_FLASH_ATTN: 0
      LLAMA_ARG_CONT_BATCHING: 1
      LLAMA_ARG_ENDPOINT_METRICS: 1
    # --mmproj и --no-flash-attn передаются через command на случай если env не сработает
    command:
      - "--mmproj"
      - *mmproj_path
      - "--no-flash-attn"
    networks:
      - pdf-analysis-network

  pdf-document-layout-analysis-gpu:
    container_name: pdf-document-layout-analysis-gpu
    entrypoint: [ "gunicorn", "-k", "uvicorn.workers.UvicornWorker", "--chdir", "./src", "app:app", "--bind", "0.0.0.0:5060", "--timeout", "10000"]
    init: true
    restart: unless-stopped
    build:
      context: .
      dockerfile: Dockerfile
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "5060:5060"
    environment:
      RESTART_IF_NO_GPU: ${RESTART_IF_NO_GPU:-false}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      REMOTE_OCR_ENABLED: "true"
      REMOTE_OCR_BASE_URL: http://llama-cpp-ocr:8080/v1
      REMOTE_OCR_API_KEY: "not-needed"
      REMOTE_OCR_MODEL: *remote_ocr_model
      REMOTE_OCR_TEMPERATURE: "0.7"
      REMOTE_OCR_TIMEOUT_SEC: "120"
      REMOTE_OCR_MAX_CONCURRENCY: ${REMOTE_OCR_MAX_CONCURRENCY:-2}
    networks:
      - pdf-analysis-network

networks:
  pdf-analysis-network:
    driver: bridge

